{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Big Data for Public Policy\n",
    "\n",
    "## Machine Learning - Classifications\n",
    "\n",
    "## [Malka Guillot]()\n",
    "\n",
    "## ETH ZÃ¼rich | [860-0033-00L](https://malkipp.github.io/big_data_policy_2021/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification belongs like regression to the field of **supervised learning**. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<i class=\"fa fa-info-circle\"></i>&nbsp; \n",
    "<strong> Classification predicts categories</strong> \n",
    "in contrast to regression which predicts numerical values.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<i class=\"fa fa-info-circle\"></i>&nbsp; \n",
    "    Other differences are:\n",
    "\n",
    "* Accuracy is measured differently\n",
    "\n",
    "\n",
    "* Other algorithms\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# IGNORE THIS CELL WHICH CUSTOMIZES LAYOUT AND STYLING OF THE NOTEBOOK !\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings = lambda *a, **kw: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple classifier: initial example\n",
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data=os.path.dirname(os.getcwd())+'/data/' \n",
    "# read some data\n",
    "beer_data = pd.read_csv(data+\"beers.csv\")\n",
    "print(beer_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# show first 5 rows\n",
    "beer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show basic statistics of the data\n",
    "beer_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Prepare data: split features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all columns up to the last one:\n",
    "X = beer_data.iloc[:, :-1]\n",
    "# only the last column:\n",
    "y = beer_data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression() \n",
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  <span style='color:green'>Your turn: `fit` & `predict` the classifier </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict` function returns a class decision using the rule $f(x)>0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# scores\n",
    "y_scores = classifier.fit(X, y).decision_function(X)\n",
    "y_scores[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`decision_function` returns the linear combination of X & estimated coefficient (term inside the exponential of the Logistic regression): $d_i=\\sum_{i=0}^p \\beta_i x_i$\n",
    "\n",
    "= **hyperplane** $\\sum_{i=0}^p \\beta_i x_i = 0$ $\\rightarrow$ $d_i$negative / positive = 2 sides of the hyperplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(len(y), \"examples\")\n",
    "print(sum(y_pred == y), \"labeled correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<i class=\"fa fa-info-circle\"></i>\n",
    "<code>y_pred == y</code> evaluates to a vector of <code>True</code> or <code>False</code> Boolean values. When used as numbers, Python handles <code>True</code> as <code>1</code> and <code>False</code> as <code>0</code>. So, <code>sum(...)</code> simply counts the correctly predicted labels.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Metrics for evaluating the performance of a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn.metrics` contains many metrics like `precision_score` etc., `confusion_matrix` prints what it means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (precision_score, recall_score, f1_score, \n",
    "                             confusion_matrix, accuracy_score, roc_curve, auc, roc_auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After applying a classifier to a data set with known labels `0` and `1`:\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<div style=\"font-size: 150%;\"><i class=\"fa fa-info-circle\"></i>&nbsp;Definition</div>\n",
    "<ul>\n",
    "\n",
    "<li><strong>TP (true positives)</strong>: labels which were predicted as <code>1</code> and actually are <code>1</code>. <br/><br/>\n",
    "\n",
    "\n",
    "<li><strong>TN (true negatives)</strong>: labels which were predicted as <code>0</code> and actually are <code>0</code>.<br/><br/>\n",
    "\n",
    "\n",
    "<li><strong>FP (false positives)</strong>: labels which were predicted as <code>1</code> and actually are <code>0</code>.<br/><br/>\n",
    "\n",
    "\n",
    "<li><strong>FN (false negatives)</strong>: labels which were predicted as <code>0</code> and actually are <code>1</code>.<br/><br/>\n",
    "\n",
    "</ul>\n",
    "\n",
    "To memorize this: \n",
    "\n",
    "<ul>\n",
    "\n",
    "<li>The second word \"positives\"/\"negatives\" refers to the prediction computed by the classifier.\n",
    "<li>The first word \"true\"/\"false\" expresses if the classification was correct or not.\n",
    "\n",
    "</ul>\n",
    "\n",
    "This is the so called <strong>Confusion Matrix</strong>:\n",
    "\n",
    "<table style=\"border: 1px; font-family: 'Source Code Pro', monocco, Consolas, monocco, monospace;\n",
    "              font-size:110%;\">\n",
    "    <tbody >\n",
    "        <tr>\n",
    "            <td style=\"padding: 10px; background:#f8f8f8;\"> </td>\n",
    "            <td style=\"padding: 10px; background:#f8f8f8;\">Predicted N</td>\n",
    "            <td style=\"padding: 10px; background:#f8f8f8;\">Predicted P</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"padding: 10px; background:#f8f8f8;\">Actual N</td>\n",
    "            <td style=\"padding: 10px; background:#fcfcfc; text-align:center; font-weight: bold\">TN         </td>\n",
    "            <td style=\"padding: 10px; background:#fcfcfc; text-align:center; font-weight: bold\">FP         </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"padding: 10px; background:#f8f8f8;\">Actual P</td>\n",
    "            <td style=\"padding: 10px; background:#fcfcfc; text-align:center; font-weight: bold\">FN         </td>\n",
    "            <td style=\"padding: 10px; background:#fcfcfc; text-align:center; font-weight: bold\">TP         </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "- So the total number of predictions can be expressed as `TP` + `FP` + `FN` + `TN`.\n",
    "\n",
    "\n",
    "- The number of correct predictions is `TP` + `TN`.\n",
    "\n",
    "\n",
    "- `TP` + `FN` is the number of positive examples in our data set, \n",
    "\n",
    "\n",
    "- `FP` + `TN` is the number of negative examples.\n",
    "\n",
    "- **precision** is computed as <code>TP / (TP + FP)</code>.\n",
    "\n",
    "\n",
    "- **recall** is computed as <code>TP / (TP + FN)</code>.\n",
    "\n",
    "- The **F1 score** is computed as <code>F1 = 2 * (precision * recall) / (precision + recall)</code>.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<div style=\"font-size: 150%;\"><i class=\"fa fa-info-circle\"></i>&nbsp;Definition</div>\n",
    "\n",
    "This allows us to define <strong>accuracy</strong> as (<code>TP</code> + <code>TN</code>) / (<code>TP</code> + <code>FP</code> + <code>FN</code> + <code>TN</code>).\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# using the results of the simple classifier of part 1\n",
    "print(confusion_matrix(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first argument of the metrics functions is the exact labels, \n",
    "# the second argument is the predictions:\n",
    "\n",
    "print(\"{:20s} {:.3f}\".format(\"precision\", precision_score(y, y_pred)))\n",
    "print(\"{:20s} {:.3f}\".format(\"recall\", recall_score(y, y_pred)))\n",
    "print(\"{:20s} {:.3f}\".format(\"f1\", f1_score(y, y_pred)))\n",
    "print(\"{:20s} {:.3f}\".format(\"accuracy\", accuracy_score(y, y_pred)))\n",
    "print(\"{:20s} {:.3f}\".format(\"auc\", roc_auc_score(y, y_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " ### Two helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def samples_color(ilabels, colors=[\"steelblue\", \"chocolate\"]):\n",
    "    '''Return colors list from labels list given as indices.'''\n",
    "    return [colors[int(i)] for i in ilabels]\n",
    "\n",
    "def plot_decision_surface(\n",
    "    features_2d, labels, classifier, preprocessing=None,\n",
    "    plt=plt, marker='.', N=100, alpha=0.2, colors=[\"steelblue\", \"chocolate\"], title=None,\n",
    "    test_features_2d=None, test_labels=None, test_s=60,\n",
    "):\n",
    "    '''Plot a 2D decision surface for a already trained classifier.'''\n",
    "\n",
    "    # sanity check\n",
    "    assert len(features_2d.columns) == 2\n",
    "\n",
    "    # pandas to numpy array; get min/max values\n",
    "    xy = np.array(features_2d)\n",
    "    min_x, min_y = xy.min(axis=0)\n",
    "    max_x, max_y = xy.max(axis=0)\n",
    "\n",
    "    # create mesh of NxN points; tech: `N*1j` is spec for including max value\n",
    "    XX, YY = np.mgrid[min_x:max_x:N*1j, min_y:max_y:N*1j]\n",
    "    points = np.c_[XX.ravel(), YY.ravel()] # shape: (N*N)x2\n",
    "\n",
    "    # apply scikit-learn API preprocessing\n",
    "    if preprocessing is not None:\n",
    "        points = preprocessing.transform(points)\n",
    "    \n",
    "    # classify grid points\n",
    "    classes = classifier.predict(points)\n",
    "\n",
    "    # plot classes color mesh\n",
    "    ZZ = classes.reshape(XX.shape) # shape: NxN\n",
    "    plt.pcolormesh(\n",
    "        XX, YY, ZZ,\n",
    "        alpha=alpha, cmap=matplotlib.colors.ListedColormap(colors),\n",
    "    )\n",
    "    # plot points\n",
    "    plt.scatter(\n",
    "        xy[:,0], xy[:,1],\n",
    "        marker=marker, color=samples_color(labels, colors=colors),\n",
    "    );\n",
    "    # set title\n",
    "    if title:\n",
    "        if hasattr(plt, 'set_title'):\n",
    "            plt.set_title(title)\n",
    "        else:\n",
    "            plt.title(title)\n",
    "    # plot test points\n",
    "    if test_features_2d is not None:\n",
    "        assert test_labels is not None\n",
    "        assert len(test_features_2d.columns) == 2\n",
    "        test_xy = np.array(test_features_2d)\n",
    "        plt.scatter(\n",
    "            test_xy[:,0], test_xy[:,1],\n",
    "            s=test_s, facecolors='none', color=samples_color(test_labels),\n",
    "        );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An overview of classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Nearest Neighbors\n",
    "The idea is very simple: to classify a sample $x$ look for **$N$ closests samples in the training data** (by default, using the Euclidean distance) and take **majority of their labels** as a result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Demonstration using a **toy data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(data+\"xor.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_2d = df.loc[:, (\"x\", \"y\")]\n",
    "labelv = df[\"label\"]\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(features_2d.iloc[:,0], features_2d.iloc[:,1], color=samples_color(labelv));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Split train & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_2d, labelv, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fit `KNeighborsClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Let's use 5 neighbors to learn\n",
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  <span style='color:green'>Your turn: compute the train & test accuracy (you can use the `score` method) </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Plotting the decision surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plot_decision_surface(\n",
    "    features_2d, labelv, classifier,\n",
    "    test_features_2d=X_test, test_labels=y_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the plot: **the points surrounded by a circle are from the test data set** (not used for learning), all other points belong to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Changing the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors_list = [1, 10, 100]\n",
    "p_list = [1, 2] #1=Manhatan distance norm ; 2=Euclidian distance\n",
    "\n",
    "print()\n",
    "for p in p_list:\n",
    "    print('# Distance ', p)\n",
    "    print()\n",
    "    for n_neighbors in n_neighbors_list:\n",
    "        print('## Nb neighbors: ', n_neighbors)\n",
    "        # Note: increase max iterations 10x for solver's convergence\n",
    "        classifier = KNeighborsClassifier(n_neighbors=n_neighbors, p=p)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        print('test score: {:.2f}%'.format(100*classifier.score(X_test, y_test)))\n",
    "        # print('weights: ', classifier.coef_[0])\n",
    "\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.title(\"p={}, n_neighbors={}\".format(p, n_neighbors))\n",
    "        plot_decision_surface(\n",
    "            features_2d, labelv, classifier,\n",
    "            test_features_2d=X_test, test_labels=y_test,\n",
    "        )        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn `LogisticRegression` the regularization weight is passed here in \"inverse\", as a classification weight parameter `C` (default `1`), meaning that it multiplies the classification loss, not the regularization penalty:\n",
    "\n",
    "$$\\text{cost} =  \\text{C}\\cdot\\text{classification_loss} + \\text{regularization_penalty}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Demonstration using a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data+\"line_separable_2d.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2d = df.loc[:, (\"x\", \"y\")]\n",
    "labelv = df[\"label\"]\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(features_2d.iloc[:,0], features_2d.iloc[:,1], color=samples_color(labelv));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features_2d, labelv, random_state=0)\n",
    "\n",
    "classifier = LogisticRegression(C=1, random_state=0)\n",
    "y_score=classifier.fit(X_train, y_train)\n",
    "print('train score: {:.2f}%'.format(100*classifier.score(X_train, y_train)))\n",
    "print('test score: {:.2f}%'.format(100*classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<p><i class=\"fa fa-info-circle\"></i>&nbsp;\n",
    "The <strong>classification loss</strong> in logistic regression is a so called <em>negative-log likelihood</em>, i.e. a negative logarithm of the logistic probability above:\n",
    "<p/>\n",
    "    \n",
    "<p>\n",
    "$$ \\text{classification_loss} = -\\log(p(x^k; p^k)) = \\log{\\left(1+\\exp{\\left(y^k\\left(b - \\sum_{i=1}^{n} w_i x_i^k\\right)\\right)}\\right)}$$\n",
    "<p/>\n",
    "\n",
    "<p>\n",
    "where $y^k$ is -1 or 1, representing class of $k$-th sample from the training data, corresponding, respectively, to class below and above the threshold (the separation line).\n",
    "\n",
    "The $+/-$ sign for the class penalizes missclassifications. If sample is below the threshold $\\sum_{i=1}^{n} w_i x_i^k < b$ and have the correct class $y^k = -1$, then we have $\\exp{\\left(\\text{negative value}\\right)}$ giving small loss. In case of misclassification $\\exp{\\left(\\text{positive value}\\right)}$ gives a much bigger loss.\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<p><i class=\"fa fa-info-circle\"></i>&nbsp;\n",
    "The <strong>reqularization penalty</strong> in logistic regression is a <em>norm of the learnt weights</em>, denoted as:\n",
    "\n",
    "<p>\n",
    "$$\\text{regularization_penalty} = \\left\\lVert w \\right\\rVert_p$$\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Using <em>L1 norm</em> ($p=1$, Manhatan distance from origin, which is sum of absolute weight values) is know for finding sparse solutions, i.e. eliminating features (weight equal to 0) when they are have low significance. With the default <em>L2 norm</em> ($p=2$, Euclidian distance from origin, which is square root of sum of squared weight values), weights of insignificant features would have small non-zero values instead.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "In <code>LogisticRegression</code> class, <code>penalty</code> parameter allows to specify type of norm to use.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Note that any solution weights and its threshold can be scaled to give the same result. Thus the regularization penalty not only prevents overfitting but also ensures a unique solution.\n",
    "</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the role of the classification weight parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_arr = plt.subplots(ncols=2, nrows=1, figsize=(2*5, 5))\n",
    "\n",
    "plot_decision_surface(\n",
    "    features_2d, labelv, classifier,\n",
    "    test_features_2d=X_test, test_labels=y_test,\n",
    "    plt=ax_arr[0],\n",
    "    title='C=1',\n",
    ")\n",
    "\n",
    "print('feature weights:', classifier.coef_)\n",
    "\n",
    "def plot_separation_line(features_2d, linear_classifier, plt=plt):\n",
    "    '''Plot a separation line for 2D dataset'''\n",
    "    \n",
    "    assert hasattr(linear_classifier, 'coef_') \n",
    "    \n",
    "    w = linear_classifier.coef_[0]\n",
    "    b = -linear_classifier.intercept_ # NOTE: intercept = negative threshold\n",
    "\n",
    "    # separation line: w[0] * x + w[1] * y - b == 0\n",
    "    feat_x = features_2d.iloc[:, 0]\n",
    "    x = np.linspace(np.min(feat_x), np.max(feat_x), 2)\n",
    "    y =  (b - w[0] * x) / w[1]\n",
    "    plt.plot(x, y, color='k', linestyle=':');\n",
    "\n",
    "plot_separation_line(features_2d, classifier, plt=ax_arr[0])\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "print('With C=100')\n",
    "print()\n",
    "\n",
    "classifier = LogisticRegression(C=100, random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "print('train score: {:.2f}%'.format(100*classifier.score(X_train, y_train)))\n",
    "print('test score: {:.2f}%'.format(100*classifier.score(X_test, y_test)))\n",
    "print('feature weights:', classifier.coef_)\n",
    "\n",
    "plot_decision_surface(\n",
    "    features_2d, labelv, classifier,\n",
    "    test_features_2d=X_test, test_labels=y_test,\n",
    "    plt=ax_arr[1],\n",
    "    title='C=100',\n",
    ")\n",
    "plot_separation_line(features_2d, classifier, plt=ax_arr[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `C=100` => the model tries hard to get all training points correctly classified, whereas with \n",
    "* `C=1` => we allow misclassification in training, in order to possibly get more general model and avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM\n",
    "\n",
    "Support-Vector Machine (SVM) classifier tries to separate two classes with a line by **finding data points (support vectors) lying closest to the separation plane**. These points determine separation plane (weights and threshold/intercept).\n",
    "\n",
    "The weights are learned such that the **margin between support vectors of different classes is maximized**.\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"./images/svm_margin.png\" width=400px></td></tr>\n",
    "    <tr><td><center><sub>Source: <a href=\"https://en.wikipedia.org/wiki/Support-vector_machine\">https://en.wikipedia.org/wiki/Support-vector_machine</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "Like in linear regression the classification is based on a weighted sum of the features (and margin maximization corresponds to minimization of the regularization penalty). \n",
    "\n",
    "Analogously to the Nearest Neighbors method the data points (support vectors) decide the class of a new data sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Demonstration: linear separable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data+\"line_separable_2d.csv\")\n",
    "features_2d = df.loc[:, (\"x\", \"y\")]\n",
    "labelv = df[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_2d, labelv, random_state=0)\n",
    "\n",
    "classifier = LinearSVC(C=1)\n",
    "classifier.fit(X_train, y_train)\n",
    "print('train score: {:.2f}%'.format(100*classifier.score(X_train, y_train)))\n",
    "print('test score: {:.2f}%'.format(100*classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_arr = plt.subplots(ncols=2, nrows=1, figsize=(2*5, 5))\n",
    "                                                      \n",
    "plot_decision_surface(\n",
    "    features_2d, labelv, classifier,\n",
    "    test_features_2d=X_test, test_labels=y_test,\n",
    "    plt=ax_arr[0],\n",
    "    title='C=1', \n",
    ")\n",
    "\n",
    "print(\"feature weights:\", classifier.coef_)\n",
    "\n",
    "def plot_margins(features_2d, linear_classifier, plt=plt):\n",
    "    '''Plot a separation line and margin lines for 2D dataset'''\n",
    "    \n",
    "    assert hasattr(linear_classifier, 'coef_') \n",
    "    \n",
    "    w = linear_classifier.coef_[0]\n",
    "    b = -linear_classifier.intercept_ # NOTE: intercept = negative threshold\n",
    "\n",
    "    # separation line: w[0] * x + w[1] * y - b == 0\n",
    "    feat_x = features_2d.iloc[:, 0]\n",
    "    x = np.linspace(np.min(feat_x), np.max(feat_x), 2)\n",
    "    y =  (b - w[0] * x) / w[1]\n",
    "    plt.plot(x, y, color='k', linestyle=':');\n",
    "\n",
    "    # margin lines: w[0] * x + w[1] * y - b == +/-1\n",
    "    y =  ((b - 1) - w[0] * x) / w[1]\n",
    "    plt.plot(x, y, color='r', linestyle=':');\n",
    "    y =  ((b + 1) - w[0] * x) / w[1]\n",
    "    plt.plot(x, y, color='r', linestyle=':');\n",
    "\n",
    "plot_margins(features_2d, classifier, plt=ax_arr[0])\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "print('With C=100')\n",
    "print()\n",
    "                                                      \n",
    "# higher C = more narrow (\"harder\") margin\n",
    "# Note: increase max iterations 50x for solver's convergence\n",
    "classifier = LinearSVC(C=100, max_iter=50000)\n",
    "classifier.fit(X_train, y_train)\n",
    "print('train score: {:.2f}%'.format(100*classifier.score(X_train, y_train)))\n",
    "print('test score: {:.2f}%'.format(100*classifier.score(X_test, y_test)))\n",
    "print(\"feature weights:\", classifier.coef_)\n",
    "\n",
    "plot_decision_surface(\n",
    "    features_2d, labelv, classifier,\n",
    "    test_features_2d=X_test, test_labels=y_test,\n",
    "    plt=ax_arr[1],\n",
    "    title='C=100', \n",
    ")\n",
    "plot_margins(features_2d, classifier, plt=ax_arr[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Demonstration: circle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(data+\"circle.csv\")\n",
    "features_2d = df.loc[:, (\"x\", \"y\")]\n",
    "labelv = df[\"label\"]\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(features_2d.iloc[:,0], features_2d.iloc[:,1], color=samples_color(labelv));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_2d, labelv, random_state=0)\n",
    "\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "print('score: {:.2f}%'.format(100*classifier.score(X_test, y_test)))\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plot_decision_surface(\n",
    "    features_2d, labelv, classifier,\n",
    "    test_features_2d=X_test.iloc[:,:2], test_labels=y_test,\n",
    ")\n",
    "\n",
    "print(\"feature weights:\", classifier.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Kernel based SVM\n",
    "Data is usually not at all linearily separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_2d, labelv, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  <span style='color:green'>Your turn: kernel classifier</span>\n",
    "\n",
    "<span style='color:green'>Specify kernel to rbf (i.e. radial) & a gamma of your choice</span>\n",
    "\n",
    "<span style='color:green'>Compute accuracy </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `gamma` parameter controls both size and *smoothness* of the decision surface.\n",
    "\n",
    "**`gamma` parameter is crucial for a good performance!**\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<p><i class=\"fa fa-warning\"></i>&nbsp;\n",
    "Before using <strong>kernel SVM</strong> you need to <strong>scale (normalize) your features first</strong>. This is because it relies on the \"similarity\"/\"distance\" function. Otherwise, kernel SVM might not work well.</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: mapping is implicit - feature weights are not there anymore (coef_);\n",
    "#       instead we have only support vectors (and their weights; dual_coef_).\n",
    "#\n",
    "# Let's just see how many of samples are used as support vectors for each class.\n",
    "print('#support vectors:', classifier.n_support_)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plot_decision_surface(\n",
    "    features_2d, labelv, classifier,\n",
    "    test_features_2d=X_test, test_labels=y_test,\n",
    "    title='gamma=20',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How to choose `gamma`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 'rbf'\n",
    "gammas = [0.05, 0.5, 5, 50, 'scale',]\n",
    "\n",
    "n = len(gammas)\n",
    "m = 1\n",
    "fig, ax_arr = plt.subplots(ncols=n, nrows=m, figsize=(4*n, 4*m))\n",
    "\n",
    "for i, gamma in enumerate(gammas):\n",
    "    classifier = SVC(kernel=kernel, gamma=gamma)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    iax = ax_arr[i]\n",
    "    iax.set_title(\"gamma: \" + str(gamma))\n",
    "    iax.set_xlabel(\n",
    "        'score: {:.2f}%\\n#support vectors: {}'.format(\n",
    "            100*classifier.score(X_test, y_test),\n",
    "            classifier.n_support_,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    plot_decision_surface(\n",
    "        features_2d, labelv, classifier,\n",
    "        test_features_2d=X_test, test_labels=y_test,\n",
    "        plt=iax,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which kernels do work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid',]\n",
    "\n",
    "df = pd.read_csv(data+\"xor.csv\")\n",
    "features_2d = df.loc[:, (\"x\", \"y\")]\n",
    "labelv = df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_2d, labelv, random_state=0)\n",
    "\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid',]\n",
    "gamma = 'scale'\n",
    "\n",
    "n = len(kernels)\n",
    "m = 1\n",
    "fig, ax_arr = plt.subplots(ncols=n, nrows=m, figsize=(4*n, 4*m))\n",
    "\n",
    "for j, kernel in enumerate(kernels):\n",
    "    classifier = SVC(kernel=kernel, gamma='scale')\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    iax = ax_arr[j]\n",
    "    iax.set_title(kernel)\n",
    "    iax.set_xlabel(\n",
    "        'score: {:.2f}%\\n#support vectors: {}'.format(\n",
    "            100*classifier.score(X_test, y_test),\n",
    "            classifier.n_support_,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    plot_decision_surface(\n",
    "        features_2d, labelv, classifier,\n",
    "        test_features_2d=X_test, test_labels=y_test,\n",
    "        plt=iax,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Outline",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "239.167px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
