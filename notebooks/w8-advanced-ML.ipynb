{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### If you are here in advance\n",
    "- Download the `w8-advanced-ML.ipynb` + data `Hitters.csv` from moodle or Github\n",
    "- `Hitters.csv` should be in a `data` folder in the parent folder of your notebook folder\n",
    "- It's better to be up to date with the github folder => this way you have the pictures in your notebook\n",
    "- Install `xgboost`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Big Data for Public Policy\n",
    "\n",
    "## Advanced Machine Learning\n",
    "\n",
    "## [Malka Guillot]()\n",
    "\n",
    "## ETH Zürich | [860-0033-00L](https://malkipp.github.io/big_data_policy_2021/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <i class=\"fas fa-video\"></i> Turn on recording\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- .slide: id=\"prologue\"-->\n",
    "## Prologue\n",
    "<html><div style='float:left'></div><hr color='#EB811B' size=2px width=796px></html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#If-you-are-here-in-advance\" data-toc-modified-id=\"If-you-are-here-in-advance-0.1\">If you are here in advance</a></span></li></ul></li><li><span><a href=\"#Advanced-Machine-Learning\" data-toc-modified-id=\"Advanced-Machine-Learning-1\">Advanced Machine Learning</a></span></li><li><span><a href=\"#Malka-Guillot\" data-toc-modified-id=\"Malka-Guillot-2\"><a href=\"\" target=\"_blank\">Malka Guillot</a></a></span></li><li><span><a href=\"#ETH-Zürich-|-860-0033-00L\" data-toc-modified-id=\"ETH-Zürich-|-860-0033-00L-3\">ETH Zürich | <a href=\"https://malkipp.github.io/big_data_policy_2021/\" target=\"_blank\">860-0033-00L</a></a></span></li><li><span><a href=\"#Prologue\" data-toc-modified-id=\"Prologue-4\">Prologue</a></span><ul class=\"toc-item\"><li><span><a href=\"#Last-2-Weeks\" data-toc-modified-id=\"Last-2-Weeks-4.1\">Last 2 Weeks</a></span></li><li><span><a href=\"#Today\" data-toc-modified-id=\"Today-4.2\">Today</a></span></li></ul></li><li><span><a href=\"#Introduction:-Ensemble-Learning\" data-toc-modified-id=\"Introduction:-Ensemble-Learning-5\">Introduction: Ensemble Learning</a></span></li><li><span><a href=\"#XGBoost-Oveview\" data-toc-modified-id=\"XGBoost-Oveview-6\">XGBoost Oveview</a></span></li><li><span><a href=\"#XGBoost-Ingredient-1---Decision-Trees\" data-toc-modified-id=\"XGBoost-Ingredient-1---Decision-Trees-7\">XGBoost Ingredient 1 - Decision Trees</a></span></li><li><span><a href=\"#Decision-Trees\" data-toc-modified-id=\"Decision-Trees-8\">Decision Trees</a></span></li><li><span><a href=\"#Example\" data-toc-modified-id=\"Example-9\">Example</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prediction-the-salary-of-baseball-players\" data-toc-modified-id=\"Prediction-the-salary-of-baseball-players-9.1\">Prediction the salary of baseball players</a></span></li><li><span><a href=\"#Let's-predict-the-log-salary-based-on\" data-toc-modified-id=\"Let's-predict-the-log-salary-based-on-9.2\">Let's predict the log salary based on</a></span></li><li><span><a href=\"#A-Visual-Representation-of-the-Data\" data-toc-modified-id=\"A-Visual-Representation-of-the-Data-9.3\">A Visual Representation of the Data</a></span></li><li><span><a href=\"#First-Split\" data-toc-modified-id=\"First-Split-9.4\">First Split</a></span></li><li><span><a href=\"#Second-Split\" data-toc-modified-id=\"Second-Split-9.5\">Second Split</a></span></li><li><span><a href=\"#Split-Surface\" data-toc-modified-id=\"Split-Surface-9.6\">Split Surface</a></span></li><li><span><a href=\"#Pros-and-Cons\" data-toc-modified-id=\"Pros-and-Cons-9.7\">Pros and Cons</a></span></li><li><span><a href=\"#Extension:-bagging,-random-forests,-and-boosting.\" data-toc-modified-id=\"Extension:-bagging,-random-forests,-and-boosting.-9.8\">Extension: bagging, random forests, and boosting.</a></span></li></ul></li><li><span><a href=\"#XGBoost-Ingredient-2----Bootstrapping\" data-toc-modified-id=\"XGBoost-Ingredient-2----Bootstrapping-10\">XGBoost Ingredient 2 -- Bootstrapping</a></span></li><li><span><a href=\"#Bagging\" data-toc-modified-id=\"Bagging-11\">Bagging</a></span><ul class=\"toc-item\"><li><span><a href=\"#=-Voting-Classifiers-+-Bootstrapping\" data-toc-modified-id=\"=-Voting-Classifiers-+-Bootstrapping-11.1\">= Voting Classifiers + Bootstrapping</a></span></li><li><span><a href=\"#Training-diverse-classifiers\" data-toc-modified-id=\"Training-diverse-classifiers-11.2\">Training diverse classifiers</a></span></li><li><span><a href=\"#Voting-Classifier\" data-toc-modified-id=\"Voting-Classifier-11.3\">Voting Classifier</a></span></li><li><span><a href=\"#Hard-voting-classifier-predictions\" data-toc-modified-id=\"Hard-voting-classifier-predictions-11.4\">Hard voting classifier predictions</a></span></li><li><span><a href=\"#Voting-Classifier\" data-toc-modified-id=\"Voting-Classifier-11.5\">Voting Classifier</a></span></li><li><span><a href=\"#Classifier’s-accuracy-on-the-test-set\" data-toc-modified-id=\"Classifier’s-accuracy-on-the-test-set-11.6\">Classifier’s accuracy on the test set</a></span></li><li><span><a href=\"#Bootstrapping\" data-toc-modified-id=\"Bootstrapping-11.7\">Bootstrapping</a></span></li><li><span><a href=\"#Bootstrapping:-Sampling-&amp;-Training\" data-toc-modified-id=\"Bootstrapping:-Sampling-&amp;-Training-11.8\">Bootstrapping: Sampling &amp; Training</a></span></li><li><span><a href=\"#Bootstrapping:-Predicting:\" data-toc-modified-id=\"Bootstrapping:-Predicting:-11.9\">Bootstrapping: Predicting:</a></span></li><li><span><a href=\"#In-sklearn:-BaggingClassifier\" data-toc-modified-id=\"In-sklearn:-BaggingClassifier-11.10\">In <code>sklearn</code>: <code>BaggingClassifier</code></a></span></li><li><span><a href=\"#Bootstrapping-Benefits\" data-toc-modified-id=\"Bootstrapping-Benefits-11.11\">Bootstrapping Benefits</a></span></li></ul></li><li><span><a href=\"#XGBoost-Ingredient-3----Random-Forest\" data-toc-modified-id=\"XGBoost-Ingredient-3----Random-Forest-12\">XGBoost Ingredient 3 -- Random Forest</a></span><ul class=\"toc-item\"><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-12.1\">Random Forest</a></span></li><li><span><a href=\"#Random-Forest-classifier-with-500-trees-(each-limited-to-maximum-16-nodes):\" data-toc-modified-id=\"Random-Forest-classifier-with-500-trees-(each-limited-to-maximum-16-nodes):-12.2\">Random Forest classifier with 500 trees (each limited to maximum 16 nodes):</a></span></li><li><span><a href=\"#-Your-turn:-Random-forest-regression-on-the-baseball-data\" data-toc-modified-id=\"-Your-turn:-Random-forest-regression-on-the-baseball-data-12.3\"><span style=\"color: green\"> Your turn: Random forest regression on the baseball data</span></a></span><ul class=\"toc-item\"><li><span><a href=\"#Going-further:\" data-toc-modified-id=\"Going-further:-12.3.1\"><span style=\"color: green\">Going further:</span></a></span></li></ul></li><li><span><a href=\"#Feature-Importance-[Aside]\" data-toc-modified-id=\"Feature-Importance-[Aside]-12.4\">Feature Importance [Aside]</a></span></li></ul></li><li><span><a href=\"#XGBoost-Ingredient-4----Boosting\" data-toc-modified-id=\"XGBoost-Ingredient-4----Boosting-13\">XGBoost Ingredient 4 -- Boosting</a></span><ul class=\"toc-item\"><li><span><a href=\"#Boosting\" data-toc-modified-id=\"Boosting-13.1\">Boosting</a></span></li></ul></li><li><span><a href=\"#XGBoost-Ingredient-5----Gradient-Boosting\" data-toc-modified-id=\"XGBoost-Ingredient-5----Gradient-Boosting-14\">XGBoost Ingredient 5 -- Gradient Boosting</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gradient-boosting-=-an-additive-ensemble-of-trees\" data-toc-modified-id=\"Gradient-boosting-=-an-additive-ensemble-of-trees-14.1\">Gradient boosting = an additive ensemble of trees</a></span></li></ul></li><li><span><a href=\"#XGBoost-Ingredient-6----XGBoost\" data-toc-modified-id=\"XGBoost-Ingredient-6----XGBoost-15\">XGBoost Ingredient 6 -- <code>XGBoost</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#XGBoost-=Extreme-Gradient-Boosting\" data-toc-modified-id=\"XGBoost-=Extreme-Gradient-Boosting-15.1\"><code>XGBoost</code> =Extreme Gradient Boosting</a></span></li><li><span><a href=\"#Feature-Importance\" data-toc-modified-id=\"Feature-Importance-15.2\">Feature Importance</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Last 2 Weeks\n",
    "- Text analysis\n",
    "\n",
    "### Today\n",
    "- Advanced Machine Learning\n",
    "    - Ensemble Learning with XGbBoost\n",
    "\n",
    "Reference:\n",
    "- [JWHT](https://static1.squarespace.com/static/5ff2adbe3fe4fe33db902812/t/601cc86d7f828c4792e0bcae/1612499080032/ISLR+Seventh+Printing.pdf), chap 8\n",
    "- [Hands-On ML](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/), chap 6, 7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!-- .slide: id=\"intro\"-->\n",
    "## Introduction: Ensemble Learning\n",
    "<html><div style='float:left'></div><hr color='#EB811B' size=2px width=796px></html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One of the most powerful supervized machine learning method\n",
    "\n",
    "- Combines serveral base model to produce one optimal predictive model     \n",
    "    $\\rightarrow $ *wisdom of the crowd*\n",
    "    \n",
    "- Focus on [XGBoost](https://arxiv.org/pdf/1603.02754.pdf): \n",
    "    - A decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework.\n",
    "    - Recent method (2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## XGBoost Oveview\n",
    "<center>\n",
    "<div class=\"r-stack\"><img src=\"images/xgboost_final.png\" style=\"height: 400px;\" > </div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XGBoost Ingredient 1 - Decision Trees\n",
    "<center>\n",
    "<div class=\"r-stack\"><img src=\"images/xgboost_1_decision_tree.png\" style=\"height: 400px;\" > </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- Decision trees **learn a series of binary splits in the data based on hard thresholds**.\n",
    "    - if yes, go right; if no, go left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Can have additional splits as you move through the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Classification trees**: \n",
    "    - when predicting a discrete variable\n",
    "    - *Example*:  predicting death after contracting Covid-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Regression trees**: \n",
    "    - when continuous a discrete variable\n",
    "    - *Example*: predicting housing prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example\n",
    "### Prediction the salary of baseball players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "df=pd.read_csv('../data/Hitters.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['Salary','Hits','Years']]\n",
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's predict the log salary based on\n",
    "- number of years of experience\n",
    "- number of hits that the player made in the previous year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y=df['Salary']\n",
    "y = np.log(df.Salary)\n",
    "X=df[['Years','Hits']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A Visual Representation of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel(\"Hits\", fontsize=16)\n",
    "plt.xlabel(\"Years\", fontsize=16)\n",
    "plt.title(\"Salary : red (law) -> green (purple)\", fontsize=18)\n",
    "plt.scatter(df['Years'], df['Hits'], c=df['Salary'], s=40,cmap='gist_rainbow')\n",
    "plt.colorbar()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### First Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree._export import plot_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Training a Decision Tree (with 2 leafs, i.e 1 split)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = DecisionTreeRegressor(max_leaf_nodes=2)\n",
    "regr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Visualizing a Decision Tree:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(regr, \n",
    "           feature_names = ['Years','Hits'], \n",
    "           class_names=['Salary'], filled = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Second Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = DecisionTreeRegressor(max_leaf_nodes=3)\n",
    "regr.fit(X, y)\n",
    "plot_tree(regr, \n",
    "           feature_names = ['Years','Hits'], \n",
    "           class_names=['Salary'], filled = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Split Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel(\"Hits\", fontsize=16)\n",
    "plt.xlabel(\"Years\", fontsize=16)\n",
    "plt.title(\"Salary : red (law) -> green (purple)\", fontsize=18)\n",
    "plt.scatter(df['Years'], df['Hits'], c=df['Salary'], s=10,cmap='gist_rainbow')\n",
    "plt.colorbar()\n",
    "plt.xticks([1, 4.5, 24])\n",
    "plt.yticks([1, 117.5, 238])\n",
    "plt.vlines(4.5, ymin=-5, ymax=250)\n",
    "plt.hlines(117.5, xmin=4.5, xmax=25)\n",
    "\n",
    "plt.annotate('R1', xy=(2,117.5), fontsize='xx-large')\n",
    "plt.annotate('R2', xy=(11,60), fontsize='xx-large')\n",
    "plt.annotate('R3', xy=(11,170), fontsize='xx-large');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pros and Cons\n",
    "\n",
    "- *Pro*: \n",
    "    - very little data preparation (no feature scaling needed)\n",
    "    - simple and useful for **interpretation**.\n",
    "\n",
    "- *Cons*: \n",
    "    - low prediction accuracy.\n",
    "    - risk of **overfit** when the tree reproduces too preciselythe training set\n",
    "    - Too many branches lower the variance but can increase thebias "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Extension: bagging, random forests, and boosting. \n",
    "\n",
    "  - These methods **grow multiple trees** which are then combined to yield a single consensus prediction.\n",
    " \n",
    "  - Combining a large number of trees $\\rightarrow$ dramatic improvements in prediction accuracy, at the expense of some loss interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XGBoost Ingredient 2 -- Bootstrapping\n",
    "<center>\n",
    "<div class=\"r-stack\"><img src=\"images/xgboost_2_bagging.png\" style=\"height: 400px;\" > </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bagging \n",
    "\n",
    "\n",
    "Combines prediction from multiple decision trees through a majority votting mechanism\n",
    "\n",
    "### = Voting Classifiers + Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training diverse classifiers\n",
    "<center>\n",
    "<div class=\"r-stack\"><img src=\"https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0701.png\" style=\"height: 300px;\" > </div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Voting Classifier\n",
    "\n",
    "- aggregate the predictions of each classifier and \n",
    "- predict the class that gets the most votes. \n",
    "\n",
    "$\\rightarrow$ Better classifier \n",
    "\n",
    "This **majority-vote classifier** is called a hard voting classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hard voting classifier predictions\n",
    "<center>\n",
    "<div class=\"r-stack\"><img src=\"https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0702.png\" style=\"height: 300px;\" > </div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Voting Classifier\n",
    "\n",
    "- generally out-perform the best classifier in the ensemble.\n",
    "- more diverse algorithms will make different types of errors, \n",
    "    - improve your ensemble’s robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=1000, noise=0.9)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Classifier’s accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, np.round(accuracy_score(y_test, y_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightarrow$ The voting classifier slightly outperforms all the individual classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bootstrapping\n",
    "\n",
    "- **Voting classifer**: use the same data on different classifiers\n",
    "\n",
    "vs. \n",
    "\n",
    "- **Bootstrapping**:\n",
    "    - Use different subsets of the data on the same classifier.\n",
    "    - I can also use different subsets of features across subclassifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bootstrapping: Sampling & Training\n",
    "<center>\n",
    "<div class=\"r-stack\"><img src=\"https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0704.png\" style=\"height: 300px;\" > </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bootstrapping: Predicting:\n",
    "\n",
    "- Prediction for a new instance by simply **aggregating the predictions** of all predictors.\n",
    "\n",
    "- Aggregating function: \n",
    "    - Regression: average prediction\n",
    "    - Classification: most frequent prediction (aka *hard voting rule*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### In `sklearn`: `BaggingClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# still using the fake blob data\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=100,\n",
    "    max_samples=50, bootstrap=True, n_jobs=-1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy of BaggingClassifier:\", np.round(accuracy_score(y_test, y_pred), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bootstrapping Benefits\n",
    "\n",
    "\n",
    "<center>\n",
    "<div class=\"r-stack\"><img src=\"https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0705.png\" style=\"height: 300px;\" > </div>\n",
    "</center>\n",
    "\n",
    "- Individual predictors have a higher bias than a predictor trained on all the data, \n",
    "- Aggregation reduces both bias and variance.\n",
    "    - Generally, the ensemble has a similar bias but lower variance than a single predictor trained on all the data.\n",
    "-  Predictors can be trained in parallel using separate CPU cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XGBoost Ingredient 3 -- Random Forest\n",
    "<center>\n",
    "<div class=\"r-stack\"><img src=\"images/xgboost_3_random_forest.png\" style=\"height: 400px;\" > </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Random Forest\n",
    "\n",
    "= an ensemble of **Decision Trees** \n",
    "\n",
    "1. Each voting tree gets its own sample of data.\n",
    "\n",
    "2. At each tree split, a random sample of features is drawn, only those features are considered for splitting.\n",
    "\n",
    "3. For each tree, error rate is computed using data outside its bootstrap sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Random Forest classifier with 500 trees (each limited to maximum 16 nodes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# still using the fake blob data\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)   \n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:green'> Your turn: Random forest regression on the baseball data</span>\n",
    "\n",
    "\n",
    "1. <span style='color:green'> Split train-test data</span>\n",
    "2. <span style='color:green'> Train the RF regression</span>\n",
    "3. <span style='color:green'>Propose a method to evaluate the model</span>\n",
    "\n",
    "#### <span style='color:green'>Going further:</span>\n",
    "- <span style='color:green'>Why removing `['Salary', 'Unnamed: 0', 'League', 'Division', 'NewLeague']` </span>\n",
    "    - <span style='color:green'>What could we do instead?</span>\n",
    "- <span style='color:green'>Graphical representation of the regression results? </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('../data/Hitters.csv')\n",
    "df=df.dropna()\n",
    "y = np.log(df.Salary)\n",
    "X = df.drop(['Salary', 'Unnamed: 0', 'League', 'Division', 'NewLeague'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set_theme(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Feature Importance [Aside]\n",
    "\n",
    "- Great proporty of random forests\n",
    "- Importance of features by looking \n",
    "    - at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest).\n",
    "\n",
    "- `SkLearn` computes this score automatically for each feature after training, \n",
    "    - results scaled to sum to 1\n",
    "    - `feature_importances_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.Series(rnd_clf.feature_importances_,index=X_train.columns).sort_values(ascending=False)\n",
    "print(feature_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XGBoost Ingredient 4 -- Boosting\n",
    "<center>\n",
    "<div class=\"r-stack\"><img src=\"images/xgboost_4_boosting.png\" style=\"height: 400px;\" > </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Boosting\n",
    "\n",
    "Any Ensemble method that can combine several weak learners into a strong learner\n",
    "\n",
    "- train predictors sequentially, each trying to correct its predecessor\n",
    "- Most used boosting methods:\n",
    "    - *Adaptive Boosting* : `AdaBoost`\n",
    "    - *Gradient Boosting*    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XGBoost Ingredient 5 -- Gradient Boosting\n",
    "<center>\n",
    "<div class=\"r-stack\"><img src=\"images/xgboost_5_gradient_boosting.png\" style=\"height: 400px;\" > </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gradient boosting = an additive ensemble of trees\n",
    "\n",
    "<center>\n",
    "<div class=\"r-stack\"><img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20200721214745/gradientboosting.PNG\" style=\"height: 400px;\" > </div>\n",
    "</center>\n",
    "\n",
    "Adds additional layers of trees to fit the residuals of the first layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- The ensemble consists of $N$ trees. \n",
    "- Tree1 is trained using the feature matrix $X$ and the labels $y$. \n",
    "- The predictions labelled $\\hat y_1$ are used to determine the training set residual errors $r_1$. \n",
    "- Tree2 is then trained using the feature matrix $X$ and the residual errors $r_1$ of Tree1 as labels. \n",
    "- The predicted results $\\hat r_1$ are then used to determine the residual $r_2$. \n",
    "- The process is repeated until all the $N$ trees forming the ensemble are trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## XGBoost Ingredient 6 -- `XGBoost`\n",
    "<center>\n",
    "<div class=\"r-stack\"><img src=\"images/xgboost_6_xgboost.png\" style=\"height: 400px;\" > </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `XGBoost` =Extreme Gradient Boosting \n",
    "\n",
    "$\\rightarrow$ optimized implementation of Gradient Boosting\n",
    "\n",
    "- easy to use\n",
    "- actively developed\n",
    "- efficient / parallelizable\n",
    "- provides model explanations\n",
    "- takes sparse matrices as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "xgb_reg=xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred=xgb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "plot_importance(xgb_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests and boosted trees provide a metric of feature importance that summarizes how well each feature contributes to predictive accuracy."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
