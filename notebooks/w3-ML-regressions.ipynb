{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Big Data for Public Policy\n",
    "\n",
    "## Machine Learning - Regressions\n",
    "\n",
    "## [Malka Guillot]()\n",
    "\n",
    "## ETH Zürich | [860-0033-00L](https://malkipp.github.io/big_data_policy_2021/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Regression belongs like classification to the field of **supervised learning**. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<i class=\"fa fa-info-circle\"></i>&nbsp; \n",
    "<strong>Regression predicts numerical values</strong> \n",
    "in contrast to classification which predicts categories.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<i class=\"fa fa-info-circle\"></i>&nbsp; \n",
    "    Other differences are:\n",
    "<ul>\n",
    "    <li> Accuracy in measured differently </li>\n",
    "    <li> Other algorithms </li>\n",
    "   </ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib notebook\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings = lambda *a, **kw: None\n",
    "\n",
    "# to make this notebook's output identical at every run\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## General ML Procedure\n",
    "\n",
    "0. Look at the data\n",
    "1. Select a ML method (eg. LASSO)\n",
    "2. Draw randomly a hold-out sample from the data\n",
    "3. Estimate the ML model using different hyperparameters\n",
    "4. Select the optimal hyperparameters\n",
    "5. Predict $\\hat Y$ using hyperparameters and extrapolated the fitted values to the retarded hold-out-sample\n",
    "6. Evaluation the prediction power of the ML in the hold-out-sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scikit-Learn Design Overview\n",
    "### Estimator: an object that can estimate parameters\n",
    "- e.g. `linear_models.LinearRegression`\n",
    "- Estimation performed by `fit()` method\n",
    "- Exogenous parameters (provided by the researcher) are called `hyperparameters`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transformer (preprocessor): An object that transforms a data set.\n",
    "- e.g. `preprocessing.StandardScaler`\n",
    "- Transformation is performed by the `transform()` method.\n",
    "- The convenience method `fit_transform()` both fits an estimator and returns the transformed input data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Predictor: An object that forms a prediction from an input data set.\n",
    "- e.g. `LinearRegression`, after training\n",
    "- The `predict()` method forms the predictions.\n",
    "- It also has a `score()` method that measures the quality of the predictions given a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Miscellaneous\n",
    "- **Inspection**: Hyperparameters and parameters are accessible. Learned parameters have an underscore suffix (e.g.`lin_reg.coef_`)\n",
    "- **Non-proliferation of classes**: Use native Python data types; existing building blocks are used as much as possible.\n",
    "- **Sensible defaults**: Provides reasonable default values for hyperparameters – easy to get a good baseline up and running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Set up and load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use as an example the **Boston housing data** (from `sklearn`), which contains 13 attributes of housing markets around Boston. The data was collected in 1978 and each of the 506 entries represent aggregated data about 14 features for homes from various suburbs in Boston, Massachusetts.\n",
    "\n",
    "The objective is to **predict the value of prices** of the house using the given features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "data = load_boston() # object is a dictionary\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Data Set Characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Create $X$ and $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full, y_full = data.data, data.target\n",
    "n_samples = X_full.shape[0]\n",
    "n_features = X_full.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_df=pd.DataFrame(X_full, columns=data['feature_names']) # to dataframe format\n",
    "X_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Look for null values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantity to predict= price (`target` or $y$) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the regression, let us inspect the features and their distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "plt.hist(y_full, bins=30)\n",
    "plt.xlabel(\"House prices in $1000\", size=15)\n",
    "plt.ylabel('count', size=15)\n",
    "plt.title('Distribution of median price in each neighborhood', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Features ($X$) used for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Distributions\n",
    "**Histogram plots** to look at the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Correlations\n",
    "\n",
    "**Boston Correlation Heatmap Example with Seaborn**\n",
    "\n",
    "The seaborn package offers a heatmap that will allow a two-dimensional graphical representation of the Boston data. The heatmap will represent the individual values that are contained in a matrix are represented as colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(rc={'figure.figsize':(8.5,5)})\n",
    "correlation_matrix = X_df.corr().round(2)\n",
    "sns.heatmap(correlation_matrix) #annot=True\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Check for multicolinearity\n",
    "\n",
    "An important point in selecting features for a linear regression model is to check for **multicolinearity**. \n",
    "\n",
    "The features RAD, TAX have a correlation of 0.91. These feature pairs are strongly correlated to each other. This can affect the model. \n",
    "\n",
    "Same goes for the features DIS and AGE which have a correlation of -0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Correlation plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(X_df[['DIS', 'AGE','RAD', 'TAX']], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Scatter plot relative to the target (price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for feature_name in X_df.columns:\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.scatter(X_df[feature_name], y_full, alpha=0.1)\n",
    "    plt.ylabel('Price', size=15)\n",
    "    plt.xlabel(feature_name, size=15)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What can we say ? \n",
    "\n",
    "- The prices increase as the value of RM increases linearly. There are few outliers and the data seems to be capped at 50.\n",
    "\n",
    "- The prices tend to decrease with an increase in LSTAT. Though it doesn’t look to be following exactly a linear line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prepare the data for ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Drop some labeled observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the observations with price >=50 (because of the right censure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=y_full<50\n",
    "\n",
    "y_full=y_full[mask==True]\n",
    "X_full=X_full[mask==True]\n",
    "X_df=X_df[mask==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Split train test sets \n",
    "### Using `train_test_split`\n",
    "Pure ramdomness of the sampling method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full,test_size=0.2, random_state=1)\n",
    "print(\"train data\", X_train.shape, y_train.shape)\n",
    "print(\"test data\", X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data cleaning\n",
    "The missing features should be:\n",
    "1. dropped\n",
    "2. imputed to some value (zero, the mean, the median...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Feature Scaling\n",
    "Most common scaling methods: \n",
    "- **standardization**= normalization by substracting the mean and dividing by the standard deviation (values are not bounded)\n",
    "- **Min-max scaling**= normalization by substracting the minimum and dividing by the maximum (values between `0` and `1`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Select and Train a Model\n",
    "**Regression algorithm** (we consider firs the `LinearRegression`, more algorithms will be discussed later):\n",
    "### First algorithm: Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our first machine learning model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <i class=\"fa fa-warning\"></i>&nbsp;<strong><code>scikit-learn</code> API</strong>\n",
    "\n",
    "In <code>scikit-learn</code> all regression algorithms have:\n",
    "<ul>\n",
    "    <li>a <strong><code>fit()</code></strong> method to learn from data, and</li>\n",
    "    <li>and a subsequent <strong><code>predict()</code></strong> method for predicting numbers from input features.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "lin_reg.fit(X_train, y_train)\n",
    "print(\"R-squared for training dataset:{}\".\n",
    "      format(np.round(lin_reg.score(X_train, y_train), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.fit(X_train_scaled, y_train)\n",
    "print(\"R-squared for training dataset & scaled features:{}\".\n",
    "      format(np.round(lin_reg.score(X_train_scaled, y_train), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coefficients of the linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(X_df.columns)\n",
    "\n",
    "print('The coefficients of the features from the linear model:')\n",
    "print(dict(zip(features, [round(x, 2) for x in lin_reg.coef_])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Metrics / error measures   \n",
    "\n",
    "`scikit-learn` offers the following metrics for measuring regression quality:\n",
    "\n",
    "### Mean absolute error\n",
    "\n",
    "Taking absolute values before adding up the deviatons assures that deviations with different signs can not cancel out.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <i class=\"fa fa-info-circle\"></i>&nbsp; <strong>mean absolute error</strong> is defined as \n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\left(\\, |y_1 - \\hat{y}_1| \\, + \\, |y_2 - \\hat{y}_2| \\, + \\, \\ldots \\,+ \\,|y_n - \\hat{y}_n| \\,\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "`neg_mean_absolute_error` in `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Mean squared error\n",
    "\n",
    "Here we replace the absolute difference by its squared difference. Squaring also insures positive differeces.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <i class=\"fa fa-info-circle\"></i>&nbsp; <strong>mean squared error</strong> is defined as \n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\left(\\, (y_1 - \\hat{y}_1)^2 \\, + \\, (y_2 - \\hat{y}_2)^2 \\, \\, \\ldots \\,+ \\,(y_n - \\hat{y}_n)^2 \\,\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "This measure is more sensitive to **outliers**: A few larger differences contribute more significantly to a larger mean squared error. \n",
    "\n",
    "`neg_mean_squared_error` in `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Median absolute error\n",
    "\n",
    "Here we replace mean calculation by median. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <i class=\"fa fa-info-circle\"></i>&nbsp; <strong>median absolute error</strong> is defined as \n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{median}\\left(\\,|y_1 - \\hat{y}_1|, \\,|y_2 - \\hat{y}_2|, \\,\\ldots, \\,|y_n - \\hat{y}_n| \\, \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "This measure is less sensitive to outliers: A few larger differences will not contribute significantly to a larger error value. \n",
    "\n",
    "`neg_median_absolute_error` in `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Mean squared log error\n",
    "\n",
    "The formula for this metric can be found [here](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-log-error). \n",
    "\n",
    "This metric is recommended **when your target values are distributed over a huge range of values**, like popoluation numbers. \n",
    "\n",
    "The previous error metrics would put a larger weight on large target values. \n",
    "\n",
    "The name is `neg_mean_squared_log_error`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### In-sample performance with MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_train_pred = lin_reg.predict(X_train_scaled)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "print(\"RMSE: %s\" % train_rmse) # = np.sqrt(np.mean((predicted - expected) ** 2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn</h3>\n",
    "    Compute:\n",
    "    <ul>\n",
    "        <li>the out-of-sample mean squarred error\n",
    "        </li>\n",
    "        <li> mean  absolute error (using $mean\\ absolute\\ error$ from $sklearn.metrics$: you need to import the function)\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Explained variance and $R^2$-score\n",
    "\n",
    "Two other scores to mention are *explained variance* and $R^2$-score. For both larger values indicate better regression results.\n",
    "\n",
    "The $R^2$-score corresponds to **the proportion of variance (of $y$) that has been explained by the independent variables in the model**. \n",
    "It takes values in the range $0 .. 1$. The name within `scikit-learn` is `R2`. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <i class=\"fa fa-info-circle\"></i>&nbsp; <strong>$R^2$</strong> is defined as \n",
    "\n",
    "$$\n",
    "R^2= 1-\\frac{\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "The formula for [explained variance](https://scikit-learn.org/stable/modules/model_evaluation.html#explained-variance-score), the score takes values up to $1$. The name within `scikit-learn` is `explained_variance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2=round(r2_score(y_test, y_test_pred), 2)\n",
    "print(\"R2: %s\" % r2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Binned Regression Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regplot\n",
    "g=sns.regplot(x= y_test_pred, y=y_test, x_bins=100)\n",
    "g=g.set_title(\"Test sample\")\n",
    "\n",
    "plt.xlabel(\"Predicted prices: $\\hat{Y}_i$\")\n",
    "plt.ylabel(\"Prices: $Y_i$\")\n",
    "plt.annotate('R2={}'.format(r2),\n",
    "            xy=(1, 0),  xycoords='axes fraction',\n",
    "            horizontalalignment='right',\n",
    "            verticalalignment='bottom')\n",
    "plt.annotate('Notes: 100 binned',\n",
    "            xy=(0, 0),  xycoords='figure fraction',\n",
    "            horizontalalignment='left',\n",
    "            verticalalignment='bottom')\n",
    "plt.plot([0, 50], [0, 50], '--k')\n",
    "plt.axis('tight')\n",
    "plt.tight_layout()\n",
    "plt.show(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Regression Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us plot how good given and predicted values match on the training data set (sic !).\n",
    "def plot_fit_quality(values_test, predicted):\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "    x = np.arange(len(predicted))\n",
    "    plt.scatter(x, predicted - values_test, color='steelblue', marker='o') \n",
    "\n",
    "    plt.plot([0, len(predicted)], [0, 0], \"k:\")\n",
    "    \n",
    "    max_diff = np.max(np.abs(predicted - values_test))\n",
    "    plt.ylim([-max_diff, max_diff])\n",
    "    \n",
    "    plt.ylabel(\"error\")\n",
    "    plt.xlabel(\"sample id\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "\n",
    "    plt.scatter(x, (predicted - values_test) / values_test, color='steelblue', marker='o') \n",
    "    plt.plot([0, len(predicted)], [0, 0], \"k:\")\n",
    "    plt.ylim([-.5, .5])\n",
    "      \n",
    "    plt.ylabel(\"relative error\")\n",
    "    plt.xlabel(\"sample id\")\n",
    "\n",
    "plot_fit_quality(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn</h3>\n",
    "    Train a ridge model and look at the goodeness of fit\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The coefficients of the features from the Ridge model:')\n",
    "print(dict(zip(features, [round(x, 2) for x in ridge_reg.coef_])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features=PolynomialFeatures(degree=2)\n",
    "X_train_poly=poly_features.fit_transform(X_train)\n",
    "X_test_poly=poly_features.fit_transform(X_test)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_poly, y_train)\n",
    "\n",
    "y_test_pred = lin_reg.predict(X_test_poly)\n",
    "test_rmse = mean_squared_error(y_test,y_test_pred)\n",
    "test_rmse = np.sqrt(test_rmse)\n",
    "print(\"test RMS: %s\" % test_rmse) \n",
    "print(\"train R2: %s\" % round(r2_score(y_train, y_train_pred), 2)) \n",
    "print(\"test R2: %s\" % round(r2_score(y_test, y_test_pred), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg=Lasso(alpha=1)       \n",
    "lasso_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = lasso_reg.predict(X_test)        \n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "print(\"test RMS: %s\" % test_rmse) \n",
    "print(\"train R2: %s\" % round(r2_score(y_train, y_train_pred), 2)) \n",
    "print(\"test R2: %s\" % round(r2_score(y_test, y_test_pred), 2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lasso regression -- with scaled X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg=Lasso(alpha=1)       \n",
    "lasso_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_test_pred = lasso_reg.predict(X_test_scaled)        \n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "print(\"test RMS: %s\" % test_rmse) \n",
    "print(\"train R2: %s\" % round(r2_score(y_train, y_train_pred), 2)) \n",
    "print(\"test R2: %s\" % round(r2_score(y_test, y_test_pred), 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The coefficients of the features from the Lasso model:')\n",
    "print(dict(zip(features, [round(x, 2) for x in lasso_reg.coef_])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "elanet_reg=ElasticNet(random_state=0)\n",
    "elanet_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_test_pred = elanet_reg.predict(X_test_scaled)        \n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "print(\"test RMS: %s\" % test_rmse) \n",
    "print(\"train R2: %s\" % round(r2_score(y_train, y_train_pred), 2)) \n",
    "print(\"test R2: %s\" % round(r2_score(y_test, y_test_pred), 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The coefficients of the features from the Lasso model:')\n",
    "print(dict(zip(features, [round(x, 2) for x in elanet_reg.coef_])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setting the **regularization parameter**: generalized Cross-Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas=np.logspace(-6, 6, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lassocv_reg = linear_model.LassoCV(alphas=alphas)\n",
    "lassocv_reg.fit(X_train, y_train)\n",
    "alpha=lassocv_reg.alpha_ \n",
    "print(\"Best alpha\", alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Then re-run the model using the best $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg=Lasso(alpha=alpha)       \n",
    "\n",
    "lasso_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_train_pred=lasso_reg.predict(X_train_scaled)\n",
    "y_test_pred = lasso_reg.predict(X_test_scaled)        \n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "print(\"test RMS: %s\" % test_rmse) \n",
    "print(\"train R2: %s\" % round(r2_score(y_train, y_train_pred), 2)) \n",
    "print(\"test R2: %s\" % round(r2_score(y_test, y_test_pred), 2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fine-tuning of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model Evaluation using Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <i class=\"fa fa-info-circle\"></i>&nbsp; cross_val_score expect a utility function rather than a cost function: the scoring function is the opposite of the MSE. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "# Perform 6-fold cross validation\n",
    "scores = cross_val_score(elanet_reg, X_train_scaled, y_train, \n",
    "                         scoring=\"neg_mean_squared_error\", cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Make cross validated predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_cv = cross_val_predict(elanet_reg, X_train_scaled, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## plt.figure(figsize=(4, 3))\n",
    "plt.scatter(y_train, y_train_pred_cv)\n",
    "plt.plot([0, 50], [0, 50], '--k')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('True price ($1000s)')\n",
    "plt.ylabel('Predicted price ($1000s)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy =r2_score(y_train, y_train_pred_cv)\n",
    "print('Cross-Predicted Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "  {'alpha': [0.0001, 0.001, 0.01, 0.1 ,1, 10],\n",
    "      'l1_ratio':[.1,.5,.9,1]}\n",
    "    \n",
    "]\n",
    "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
    "grid_search = GridSearchCV(elanet_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The best hyperparameter combination found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Score of each hyperparameter combination tested during the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### In a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_cvres=pd.DataFrame(cvres)\n",
    "df_cvres['mean_test_score_pos_sqrt']=df_cvres['mean_test_score'].apply(lambda x: np.sqrt(-x))\n",
    "df_cvres['log_param_alpha']=df_cvres['param_alpha'].apply(lambda x: np.log(x))\n",
    "df_cvres.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Vizualize the grid search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,1)\n",
    "plt.plot(df_cvres[\"log_param_alpha\"], df_cvres[\"mean_test_score_pos_sqrt\"])\n",
    "ax.set_title(\"Grid Search\", fontsize=20, fontweight='bold')\n",
    "ax.set_xlabel(\"$\\log (alpha)$\", fontsize=16)\n",
    "ax.set_ylabel('Avg. mean test score', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Other possibility: for randomized search of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is not covered today:\n",
    "- more advanced regression algorithms (gradient boosting, random forest)\n",
    "- classification algorithm\n",
    "- pipelines"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
